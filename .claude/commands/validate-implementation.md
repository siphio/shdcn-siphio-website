---
description: "Scenario-based agent validation that tests all user flows, capabilities, and error paths"
argument-hint: [plan-file-path] [--full]
---

# Validate Implementation: Scenario-Based Agent Validation

## Overview

**Goes beyond code health checks.** Tests every user flow, tool capability, decision tree, and error recovery path defined in the PRD. Validates that the agent behaves correctly across all scenarios, not just that the code compiles.

**Three validation sources:**
1. **Plan file** â†’ VALIDATION COMMANDS section (code-level checks)
2. **PRD Section 4.3** â†’ Scenario definitions (agent behavior checks)
3. **Technology profiles** â†’ Validation hooks from `.agents/reference/` (integration checks)

**Philosophy**: Static analysis tells you code *looks* right. Functional testing proves it *runs*. Scenario validation proves the agent *behaves correctly*.

## Arguments

- `$ARGUMENTS`: Plan file path (optional, defaults to most recent in `.agents/plans/`)
- `--full`: Run all levels including full pipeline end-to-end (slower, may cost API credits)

### What Runs in Each Mode

| Phase | What It Does | Default | --full |
|-------|-------------|---------|--------|
| Phase 1 | Static analysis (code review) | âœ… ALWAYS | âœ… ALWAYS |
| Phase 2 | Level 1 syntax + Level 2 unit tests | âœ… ALWAYS | âœ… ALWAYS |
| Phase 3 | **LIVE integration tests (Tiers 1-3) + scenario validation** | âœ… ALWAYS | âœ… ALWAYS |
| Phase 4 | Full end-to-end pipeline run | âŒ SKIPPED | âœ… RUNS |

**CRITICAL: Phase 3 ALWAYS runs live tests.** This means:
- **Tier 1** (auto-live health checks) â€” runs automatically, no approval needed
- **Tier 2** (auto-live with test data) â€” runs automatically, no approval needed
- **Tier 3** (approval-required) â€” presents each test to user for approval before executing
- **Tier 4** (mock-only) â€” runs automatically with fixtures

The `--full` flag ONLY adds Phase 4 (full pipeline). It does NOT control whether live tests run â€” **live tests (Tiers 1-3) are the default behavior, not an opt-in.**

## Reasoning Approach

**CoT Style:** Per-subtask (one per validation level/scenario category)

For each validation level:
1. Load the relevant source (plan commands, PRD scenarios, technology profiles)
2. Determine what to test and expected outcomes
3. Execute tests and capture results
4. Compare actual vs expected outcomes
5. Classify: PASS / FAIL / PARTIAL / SKIPPED

For scenario validation specifically:
1. Map scenario Given/When/Then to executable steps
2. Determine integration tier (live vs fixture vs mock)
3. Execute and verify each assertion
4. Document deviations with specific details

## Hook Toggle

Check CLAUDE.md for `## PIV Configuration` â†’ `hooks_enabled` setting.
If arguments contain `--with-hooks`, enable hooks. If `--no-hooks`, disable.
Strip `--with-hooks`, `--no-hooks`, and `--full` from arguments before using remaining text as plan path.

---

## Architecture

```
Phase 0: Context Loading
    â”‚
    â”œâ”€â”€ Read plan file â†’ Extract VALIDATION COMMANDS
    â”œâ”€â”€ Read PRD â†’ Extract Section 4.3 Scenario Definitions
    â”œâ”€â”€ Read PRD â†’ Extract Section 4.4 Error Recovery Patterns
    â””â”€â”€ Read technology profiles â†’ Extract Validation Hooks (Section 9)

Phase 1: Static Analysis (Quick)
    â”‚
    â””â”€â”€ Brief code review for obvious issues

Phase 2: Code Validation
    â”‚
    â”œâ”€â”€ Level 1: Syntax/lint/type checks
    â””â”€â”€ Level 2: Unit + component tests

Phase 3: LIVE Integration + Scenario Validation â† THE KEY PHASE (ALWAYS RUNS)
    â”‚
    â”œâ”€â”€ Step 1: Tier 1 â€” Auto-live health checks (connectivity, auth) â† LIVE, NO APPROVAL
    â”œâ”€â”€ Step 2: Tier 2 â€” Auto-live with test data + cleanup â† LIVE, NO APPROVAL
    â”œâ”€â”€ Step 3: Tier 3 â€” Approval-required live tests â† LIVE, ASK USER FIRST
    â”œâ”€â”€ Step 4: Tier 4 â€” Mock-only tests with fixtures
    â”œâ”€â”€ Step 5: PRD scenario validation (Section 4.3) using tier results
    â””â”€â”€ Step 6: Decision tree verification (Section 4.2)

Phase 4: Full Pipeline (--full only)
    â”‚
    â””â”€â”€ End-to-end agent run with real/mock inputs
```

---

## Phase 0: Context Loading

### Step 1: Locate Plan

```bash
# If $ARGUMENTS provided (excluding flags), use it
# Otherwise find most recent:
ls -t .agents/plans/*.md 2>/dev/null | head -1
```

### Step 2: Load Validation Sources

**From Plan File:**
- Read `## VALIDATION COMMANDS` section â†’ Parse into Levels 1-5
- Read `## ACCEPTANCE CRITERIA` section â†’ Extract criteria list

**From PRD (if exists):**
- Read `## 4. Agent Behavior Specification`
- Extract Section 4.3: Scenario Definitions (all scenarios with Given/When/Then/Error/Edge)
- Extract Section 4.2: Decision Trees (expected decision outcomes)
- Extract Section 4.4: Error Recovery Patterns (expected recovery behaviors)

**From Technology Profiles (if exist):**
- Check `.agents/reference/` for `*-profile.md` files
- From each relevant profile, read Section 9: Validation Hooks
- Extract health checks, smoke tests, and mock strategies

### Step 3: Build Validation Matrix

Combine all sources into a validation matrix:

```
## Validation Starting

**Feature**: [Name from plan]
**Plan**: [path]
**PRD Available**: [Yes/No]
**Technology Profiles**: [list or "none"]

### Validation Commands (from Plan)
- Level 1 (Syntax): [N] commands
- Level 2 (Components): [N] commands

### Scenario Validation (from PRD Section 4.3)
- Happy path scenarios: [N]
- Error recovery scenarios: [N]
- Edge case scenarios: [N]
- Integration failure scenarios: [N]

### Technology Validation (from Profiles)
- Health checks: [N]
- Smoke tests: [N]

### Acceptance Criteria
- [ ] [Criterion 1]
- [ ] [Criterion 2]

---
Proceeding to validation...
```

---

## Phase 1: Static Analysis (Brief)

**Quick code review - max 5 minutes, max 10 tool calls**

Read up to 3 key files mentioned in the plan and check for:
- Missing error handling (especially for error recovery paths in PRD 4.4)
- Obvious bugs or unimplemented functions (TODO/FIXME)
- Decision tree logic that doesn't match PRD Section 4.2

Output brief findings before running tests.

---

## Phase 2: Code Validation

### Level 1: Syntax Validation

**Always run.** Execute each command from plan's Level 1:

```
Running: `[command]`
âœ… PASS: `[command]` (exit code 0)
```
or
```
âŒ FAIL: `[command]` (exit code 1)
Error: [error details]
```

**Stop validation if Level 1 fails** - no point testing broken code.

### Level 2: Component Validation

**Always run.** Execute each command from plan's Level 2.

**Handle interactive commands:** Mark as "VERIFY MANUALLY"
**Handle timeouts:** 60 second timeout, mark as âš ï¸ TIMEOUT

---

## Phase 3: Live Integration Testing & Scenario Validation

> **âš ï¸ THIS PHASE ALWAYS RUNS â€” it is NOT gated behind `--full`.**
> **THE CORE PHASE - Tests real API integrations AND agent behavior against PRD scenarios.**
> Uses the four-tier testing classification from technology profiles (Section 9).
>
> **You MUST execute Tiers 1-2 automatically (live, no approval), present Tier 3 for user approval,**
> **and run Tier 4 with fixtures. Skipping live tests or treating everything as mock-only is a validation failure.**

### Step 1: Tier 1 â€” Auto-Live Health Checks (No Approval) â€” MANDATORY

**DO NOT SKIP. Execute these LIVE against real services â€” not mocked.**
Execute ALL Tier 1 tests from every relevant technology profile automatically.
These are read-only, zero-cost operations that verify connectivity and auth.

```
### Tier 1: Live Integration Health Checks

[Technology Name]:
  Endpoint: GET /[endpoint]
  Running: [health check command from profile Section 9.1]
  Response: [actual response summary]
  Schema: [fields match / fields missing]
  âœ… HEALTHY | âŒ UNREACHABLE | âš ï¸ AUTH FAILED

[Next Technology]:
  ...
```

**If Tier 1 fails for a technology:**
- Mark ALL scenarios depending on that technology as âš ï¸ DEGRADED
- Continue with other technologies
- Attempt mock fallback for dependent scenarios if fixtures exist

### Step 2: Tier 2 â€” Auto-Live with Test Data (No Approval) â€” MANDATORY

**DO NOT SKIP. Execute these LIVE with real test data â€” not mocked.**
Execute ALL Tier 2 tests automatically using pre-defined test data from profiles.
These have controlled side effects with automatic cleanup.

```
### Tier 2: Live Tests with Test Data

[Technology Name] - [Operation]:
  Endpoint: POST /[endpoint]
  Test data: [summary of test input - e.g., "PIV_TEST_ prefixed campaign"]
  Response: [actual response]
  Schema valid: âœ… / âŒ

  Cleanup: [DELETE /endpoint/{id}]
  Cleanup result: âœ… CLEANED | âŒ CLEANUP FAILED (manual cleanup needed)
```

**Test data sourcing:**
- Read test configuration from profile Section 9.1 (Tier 2 table)
- Environment variables (PIV_TEST_EMAIL, etc.) must be set in .env
- If env vars missing: WARN and skip Tier 2 for that technology

**Cleanup is mandatory:** Always run cleanup procedures after Tier 2 tests, even if the test itself failed. Cleanup must be idempotent.

### Step 3: Tier 3 â€” Approval-Required Live Tests (Human in the Loop) â€” MANDATORY

**DO NOT SKIP. Present each test to the user for approval â€” do not silently mark as "Not applicable".**
For each Tier 3 endpoint in the technology profiles, present the user with an approval prompt BEFORE executing. **Never auto-execute Tier 3 tests.**

```
### Tier 3: Approval-Required Tests

ðŸ”” [Technology Name] - [Operation]

  To validate: [which PRD scenario this tests]
  Action: [METHOD /endpoint] with [test input description]
  Cost: [estimated cost from profile]
  Effect: [what happens - credits consumed, record created, etc.]
  Cleanup: [auto / manual / none]
  Last recorded: [date of existing fixture, or "no fixture"]

  â†’ User chose: APPROVE / USE FIXTURE / SKIP

  [If APPROVED]:
    Response: [actual response]
    Schema valid: âœ… / âŒ
    Fixture saved: .agents/fixtures/[tech]-[endpoint].json
    âœ… PASS | âŒ FAIL

  [If FIXTURE]:
    Loaded: .agents/fixtures/[tech]-[endpoint].json (recorded [date])
    Agent processed fixture: [result]
    âœ… PASS (fixture) | âŒ FAIL (fixture)

  [If SKIPPED]:
    â­ï¸ SKIPPED by user
```

**Approval interaction:**
- Present ALL Tier 3 tests at once so user can batch approve/skip
- For each, show: endpoint, cost, effect, and which scenario it validates
- Accept: approve all, skip all, or individual decisions
- Record user decisions in validation report

**Response recording:**
When user approves a live Tier 3 call:
1. Execute the call and capture full response
2. Save to `.agents/fixtures/{technology}-{endpoint-name}.json`
3. Include timestamp, request, and response
4. On future runs, offer recorded fixture as alternative to fresh call

### Step 4: Tier 4 â€” Mock-Only Tests (Automatic)

Load fixtures for Tier 4 endpoints and feed responses into agent logic.
Tests that the agent correctly processes responses, not that the API works.

```
### Tier 4: Mock-Based Validation

[Technology Name] - [Operation]:
  Fixture: .agents/fixtures/[tech]-[endpoint].json
  Agent behavior: [what agent did with the fixture data]
  Decision tree: [which PRD 4.2 decision was triggered]
  Expected outcome: [from PRD]
  Actual outcome: [what happened]
  âœ… PASS | âŒ FAIL
```

**If fixture doesn't exist:**
- WARN: "No fixture for [endpoint]. Create one by running Tier 3 approval test first, or manually add fixture."
- Mark as âš ï¸ NO FIXTURE

### Step 5: PRD Scenario Validation

Now test full agent scenarios from PRD Section 4.3 using the integration results from Steps 1-4.

For each scenario, the integration tier determines how it's tested:

```
### Scenario: [Name] (PRD 4.3)

Given: [Initial state from PRD]
APIs involved: [Technology A (Tier 1), Technology B (Tier 3)]
Integration status: [All APIs healthy / degraded / mocked]

When: [Trigger action]
Execute: [Command to trigger the agent workflow]

Then: [Expected outcome from PRD]
Verify: [Check output, state changes, API calls made]

Result: âœ… PASS | âŒ FAIL | âš ï¸ PARTIAL (some APIs mocked)
Details: [What happened, which tiers were live vs mocked]
```

**Scenario categories:**

**Happy paths** â€” Test with maximum live integration (Tiers 1-3 where approved):
- Agent receives real API responses and processes them correctly
- Verify full decision tree execution with real data shapes

**Error recovery** â€” Simulate errors using mocks even for live APIs:
- Override Tier 1/2 responses with error fixtures to test recovery
- Verify agent handles timeouts, rate limits, auth failures per PRD Section 4.4

**Edge cases** â€” Test with unusual inputs and boundary conditions:
- Feed edge case data to agent logic
- Verify graceful handling per PRD scenarios

### Step 6: Decision Tree Verification

For each decision tree in PRD Section 4.2, verify with real data where possible:

```
### Decision Tree: [Name] (PRD 4.2)

Data source: [Live Tier 1-3 response / Fixture / Mock]

| Condition | Expected Action | Actual Action | Data Source | Status |
|-----------|----------------|---------------|-------------|--------|
| [Condition A] | [Action A] | [What happened] | [Live/Fixture] | âœ…/âŒ |
| [Condition B] | [Action B] | [What happened] | [Live/Fixture] | âœ…/âŒ |
| [Failure] | [Recovery] | [What happened] | [Mock error] | âœ…/âŒ |
```

### Agent Teams Mode (When Available)

> Parallel validation across tiers and scenario categories.

```
Team Lead coordinates validation:
â”œâ”€â”€ Teammate 1: Tier 1-2 integration tests (Steps 1-2, auto)
â”œâ”€â”€ Teammate 2: Happy path scenarios (Step 5, after Tier 1-2 complete)
â”œâ”€â”€ Teammate 3: Error recovery + edge cases (Step 5)
â”œâ”€â”€ Lead: Handles Tier 3 approvals (Step 3 - requires user interaction)
â””â”€â”€ Lead: Decision tree verification + report (Step 6)
```

Tier 3 stays with the Lead because it requires human interaction. All other tiers parallelize across teammates.

---

## Phase 4: Full Pipeline (--full flag only)

**Only run if `--full` flag provided.** This may:
- Make real API calls (cost money)
- Take several minutes
- Create actual output files

```
### Full Pipeline

Running: `[end-to-end command]`
â³ Running... (this may take several minutes)
âœ… PASS: Agent completed successfully

Verifying outputs...
Running: `[output verification command]`
âœ… FOUND: [output file] ([size])
âœ… VALID: [format/quality details]
```

### Output Verification

After running commands, verify expected outputs:
- Check output files exist and are valid
- For media files: verify format with ffprobe or similar
- For data files: verify schema and content
- For API responses: verify response structure

---

## Phase 5: Report

### Write Report File

Location: `.agents/validation/{feature-name}-{YYYY-MM-DD}.md`

```markdown
# Validation Report: [Feature Name]

**Date**: [YYYY-MM-DD]
**Mode**: Standard | Full
**Duration**: [X] minutes
**PRD Scenarios Tested**: [N] of [Total]

---

## Code Validation Results

### Level 1: Syntax
| Command | Status | Details |
|---------|--------|---------|
| `[command]` | âœ… PASS | No errors |

### Level 2: Components
| Command | Status | Details |
|---------|--------|---------|
| `[command]` | âœ… PASS | [N] tests |

---

## Scenario Validation Results

### Happy Paths
| Scenario (PRD Ref) | Status | Details |
|---------------------|--------|---------|
| [Name] (SC-001) | âœ… PASS | [Brief] |

### Error Recovery
| Scenario (PRD Ref) | Status | Details |
|---------------------|--------|---------|
| [Name] (SC-005) | âœ… PASS | [Brief] |

### Edge Cases
| Scenario (PRD Ref) | Status | Details |
|---------------------|--------|---------|
| [Name] (SC-010) | âœ… PASS | [Brief] |

### Decision Trees
| Decision (PRD 4.2) | Branches Tested | Pass | Fail |
|---------------------|-----------------|------|------|
| [Name] | [N] | [N] | [N] |

---

## Technology Integration (Four-Tier Results)

### Tier 1: Auto-Live Health Checks
| Technology | Endpoint | Status | Details |
|-----------|----------|--------|---------|
| [Name] | GET /[endpoint] | âœ… HEALTHY | [Response summary] |

### Tier 2: Auto-Live with Test Data
| Technology | Operation | Status | Cleanup | Details |
|-----------|-----------|--------|---------|---------|
| [Name] | POST /[endpoint] | âœ… PASS | âœ… CLEANED | [Brief] |

### Tier 3: Approval-Required
| Technology | Operation | User Decision | Status | Fixture Saved |
|-----------|-----------|---------------|--------|---------------|
| [Name] | POST /[endpoint] | APPROVED | âœ… PASS | `.agents/fixtures/[file]` |
| [Name] | POST /[endpoint] | SKIPPED | â­ï¸ SKIP | N/A |

### Tier 4: Mock-Only
| Technology | Operation | Fixture Used | Agent Behavior | Status |
|-----------|-----------|-------------|----------------|--------|
| [Name] | POST /[endpoint] | `.agents/fixtures/[file]` | [Decision triggered] | âœ… PASS |

---

## Acceptance Criteria

- [x] [Criterion] - **VERIFIED** (Level/Scenario)
- [ ] [Criterion] - **MANUAL CHECK NEEDED**

---

## Summary

**Overall**: ðŸŸ¢ READY | ðŸŸ¡ ISSUES | ðŸ”´ BROKEN

| Category | Pass | Fail | Skip |
|----------|------|------|------|
| Syntax | [N] | [N] | [N] |
| Components | [N] | [N] | [N] |
| Happy Paths | [N] | [N] | [N] |
| Error Recovery | [N] | [N] | [N] |
| Edge Cases | [N] | [N] | [N] |
| Decision Trees | [N] | [N] | [N] |
| Tier 1 (Auto-Live) | [N] | [N] | [N] |
| Tier 2 (Test Data) | [N] | [N] | [N] |
| Tier 3 (Approval) | [N] | [N] | [N] |
| Tier 4 (Mock) | [N] | [N] | [N] |
| Pipeline (if --full) | [N] | [N] | [N] |

---

## Issues Found

[List any failures with details and suggested fixes]

## Next Steps

[Based on results - ready for /commit or needs fixes]
```

### Terminal Summary

```
## Validation Complete

**Report**: `.agents/validation/[file].md`

### Code Validation
| Level | Results |
|-------|---------|
| Syntax | âœ… [N]/[N] passed |
| Components | âœ… [N]/[N] passed |

### Scenario Validation
| Category | Results |
|----------|---------|
| Happy Paths | âœ… [N]/[N] |
| Error Recovery | âœ… [N]/[N] |
| Edge Cases | âœ… [N]/[N] |
| Decision Trees | âœ… [N]/[N] branches |

### Technology Integration (Four Tiers)
| Tier | Results |
|------|---------|
| Tier 1 (Auto-Live) | âœ… [N]/[N] healthy |
| Tier 2 (Test Data) | âœ… [N]/[N] passed, [N]/[N] cleaned |
| Tier 3 (Approval) | âœ… [N] approved, [N] skipped, [N] fixture |
| Tier 4 (Mock) | âœ… [N]/[N] passed |

### Acceptance Criteria
âœ… [N]/[N] verified

### Next Steps
â†’ Ready for `/commit` | Fix [N] issues first
```

### Reasoning

Output 4-8 bullets summarizing validation:

```
### Reasoning
- Tested [N] code validation commands (Level 1-2)
- Validated [N] PRD scenarios ([N] happy, [N] error, [N] edge)
- Verified [N] decision tree branches
- Technology integration: [N] Tier 1, [N] Tier 2, [N] Tier 3, [N] Tier 4
- Key finding: [most important result]
```

### Reflection

Self-critique the validation (terminal only):
- Did we achieve full scenario coverage from PRD Section 4.3?
- Are any decision tree branches untested?
- Were failure categories correctly identified?
- Is the recommended next step accurate given results?

### PIV-Automator-Hooks (If Enabled)

If hooks are enabled, append to the validation report file:

```
## PIV-Automator-Hooks
validation_status: [pass|partial|fail]
scenarios_passed: [N]/[Total]
scenarios_failed: [N]
decision_branches_tested: [N]/[Total]
failure_categories: [comma-separated: e.g. edge-cases,rate-limits]
suggested_action: [commit|re-execute|fix-and-revalidate]
suggested_command: [commit|execute|validate-implementation]
suggested_arg: "[appropriate argument]"
retry_remaining: [N]
requires_clear: [true|false]
confidence: [high|medium|low]
```

---

## Phase 6: Append Validation Learnings to Execution Memory

After writing the validation report, automatically append validation learnings to the corresponding execution memory file. This runs in the background â€” no user action required.

**File**: `.agents/memory/{plan-name}-execution.md` (append to existing file from `/execute`)

**If the file does not exist** (e.g., validation run independently), create it with just the validation section below.
**Create the `.agents/memory/` directory if it does not exist.**

Append the following section (10-15 lines max):

```markdown

## Validation Learnings

**Date**: {YYYY-MM-DD}
**Result**: {PASS|PARTIAL|FAIL}
**Pass Rate**: {N}/{Total} scenarios

### What Failed and Why
- [Specific failures with root cause â€” e.g., "SC-003 failed: webhook handler missing retry logic per PRD 4.4"]
- [If all passed: "All scenarios passed on first validation run"]

### Testing Patterns for Future Phases
- [Cleanup ordering, timeout adjustments, fixture requirements discovered]
- [Tier-specific learnings â€” e.g., "Tier 2 Supabase tests require 2s delay between create and read"]

### Acceptance Criteria Gaps
- [Any criteria that couldn't be verified and why]
- [If none: "All acceptance criteria verified"]
```

**Rules:**
- Only record things that would prevent the same failures in future phases
- Be specific â€” "webhook handler needs retry" is useful; "some tests failed" is not
- If validation passed cleanly, still append the section noting clean pass so future phases know the patterns work

---

## Handling Failures

### If Level 1 fails:
Stop validation. Syntax errors must be fixed first.

### If Level 2 fails:
Continue to scenario validation â€” component failures don't always block scenario testing.

### If scenarios fail:
Document which scenarios failed and what the agent actually did vs. expected.
Provide specific guidance: "PRD says agent should retry 3 times, but agent only retries once."

### If technology integration fails:
Check if service is reachable. If not, attempt mock mode.
Document whether failure is agent code issue or external service issue.

---

## Handling Different Project Types

### Agent Projects (Claude SDK / Custom)
```bash
# Level 1: Syntax
pnpm exec tsc --noEmit
# Level 2: Components
pnpm test
# Level 3: Scenarios
pnpm exec tsx -e "import('./src/agent').then(a => a.handleScenario(...))"
# Level 4: Full pipeline
pnpm dev agent -u "..." -a "..."
```

### Python Agent Projects
```bash
# Level 1: Syntax
uv run ruff check .
# Level 2: Components
uv run pytest
# Level 3: Scenarios
uv run python -m agent.test_scenarios
# Level 4: Full pipeline
uv run python -m agent.main --input "..."
```

---

## Usage

```bash
# Standard: Syntax + Components + Scenarios (Levels 1-3)
/validate-implementation

# Standard with specific plan
/validate-implementation .agents/plans/phase-2.md

# Full: Includes end-to-end pipeline (Levels 1-4)
/validate-implementation --full

# Full with specific plan
/validate-implementation .agents/plans/phase-2.md --full
```

---

## Completion Criteria

- [ ] Plan read and validation commands extracted
- [ ] PRD scenarios extracted (if PRD exists)
- [ ] Technology profiles read (if they exist)
- [ ] Level 1 commands executed (all must pass to continue)
- [ ] Level 2 commands executed (failures noted)
- [ ] Tier 1 auto-live health checks executed
- [ ] Tier 2 auto-live tests with test data executed + cleanup run
- [ ] Tier 3 approval-required tests presented to user (approved/skipped/fixture)
- [ ] Tier 4 mock-only tests executed with fixtures
- [ ] PRD scenario validation complete (happy paths, error recovery, edge cases)
- [ ] Decision tree verification complete
- [ ] Level 4 pipeline tested (if --full flag)
- [ ] Report written with actual pass/fail results per tier
- [ ] Acceptance criteria verified against scenarios
- [ ] All PRD scenarios accounted for (tested or documented as untestable)
